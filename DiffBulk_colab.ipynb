{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bochong01/DiffBulk/blob/main/DiffBulk_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYP6lWdSX3HY"
      },
      "source": [
        "# üìò DiffBulk: Enhancing Spatial Transcriptomic Prediction with Diffusion-Based Training\n",
        "\n",
        "Bochong Zhang (+,1), Tianyi Zhang (+,1,2), Qiaochu Xue (1,3), Zeyu Liu (3), Dankai Liao (1,3), Timothy Antoni (2), YEO HUI TING GRACE (2), Sicheng Chen (3), Hwee Kuan LEE (2), Shangqing Lyu (\\*,3), and Yueming Jin (\\*,1)\n",
        "\n",
        "Affiliations:\n",
        "\n",
        "(1) National University of Singapore (NUS)\n",
        "(2) Agency for Science, Technology and Research (A*STAR)\n",
        "(3) PuzzleLogic Pte Ltd\n",
        "\n",
        "(+) Authors contributed equally\n",
        "(*) Corresponding authors\n",
        "\n",
        "# üìñ About This Notebook\n",
        "\n",
        "This notebook provides the **official** Colab walkthrough of DiffBulk, a two-stage diffusion-based framework designed to learn gene-aware histology image representations for spatial transcriptomic prediction.\n",
        "\n",
        "* Suggest to use google colab pro+ (high RAM+GPU) for this run through\n",
        "\n",
        "* Our github page: https://github.com/Bochong01/DiffBulk\n",
        "\n",
        "It demonstrates:\n",
        "\n",
        "**Stage 1 ‚Äî Diffusion Pretraining**\n",
        "\n",
        "Learning conditional image representations guided by gene expression profiles.\n",
        "\n",
        "**Stage 2 ‚Äî Downstream Gene Expression Training & Evaluation**\n",
        "\n",
        "Using the pretrained diffusion encoder to train a lightweight gene prediction module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6SjPlpYWcc"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnbrNSoSXFm5",
        "outputId": "a9e15c3b-4f53-422f-f5e2-0402ea14c1f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "Mon Nov 24 14:49:52 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   35C    P0             55W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYevYeMFYmlx"
      },
      "source": [
        "## üìÅ Create File-System Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePtQFcQCEPlu",
        "outputId": "26e10df5-ab87-411b-855c-57795d84dda8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DiffBulk'...\n",
            "remote: Enumerating objects: 118, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 118 (delta 29), reused 86 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (118/118), 1.18 MiB | 52.33 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Bochong01/DiffBulk.git\n",
        "!cd /content/DiffBulk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1_HUut4YYm5"
      },
      "source": [
        "## üì• Load the Demo Data\n",
        "\n",
        "Here we load a minimal demo dataset that allows the user to:\n",
        "\n",
        "- run diffusion pretraining\n",
        "\n",
        "- train the downstream gene predictor\n",
        "\n",
        "- evaluation\n",
        "\n",
        "The demo dataset is intentionally lightweight so that the full pipeline can be executed within Colab Pro GPU limits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jDmwPLMjB_69"
      },
      "outputs": [],
      "source": [
        "# make data dir\n",
        "!mkdir -p /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "collapsed": true,
        "id": "oDcOe6MrBUVi",
        "outputId": "b577381c-d27d-490b-d8e4-92f9a65c450d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_ZxmAJD4ld2N_sXi_dOLJAoeWpOFdCY_\n",
            "From (redirected): https://drive.google.com/uc?id=1_ZxmAJD4ld2N_sXi_dOLJAoeWpOFdCY_&confirm=t&uuid=21ff3453-cbbb-4408-89ac-7a4a4fdbc5cd\n",
            "To: /content/data/DiffBulk_data.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.02G/4.02G [00:55<00:00, 72.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/data/DiffBulk_data.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "file_id = \"1_ZxmAJD4ld2N_sXi_dOLJAoeWpOFdCY_\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "output = \"/content/data/DiffBulk_data.zip\"  # rename if you like\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZCKmq5C3ybZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7167408-11bf-48ed-9afc-c35c964d737a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3.8G\n",
            "drwxr-xr-x 2 root root 4.0K Nov 14 15:35 crunchDAO\n",
            "-rw-r--r-- 1 root root 3.8G Nov 14 08:40 DiffBulk_data.zip\n",
            "drwxr-xr-x 2 root root 4.0K Nov 14 15:50 HEST_bowel\n",
            "drwxr-xr-x 2 root root 4.0K Nov 14 15:38 HEST_pancreas\n"
          ]
        }
      ],
      "source": [
        "# unzip\n",
        "!unzip -q /content/data/DiffBulk_data.zip -d /content/data\n",
        "\n",
        "# check\n",
        "!ls -lh /content/data | head -n 30\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLxxHGq_wwwL"
      },
      "source": [
        "## üóÇÔ∏è Arrange the Working Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1Yb2b6TGF4r",
        "outputId": "8a326831-3e41-44ee-a2a3-618667dfd615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DiffBulk\n"
          ]
        }
      ],
      "source": [
        "# change working dir\n",
        "import os\n",
        "os.chdir(\"/content/DiffBulk\")\n",
        "!pwd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h31KAx1ZZEl9"
      },
      "source": [
        "# üß¨ DiffBulk Pipeline Illustration\n",
        "\n",
        "The DiffBulk framework follows a two-stage training and evaluation process.\n",
        "Below is the structure demonstrated in this notebook:\n",
        "\n",
        "**1. Diffusion Pretraining**\n",
        "\n",
        "- Learns gene-aware histology image representations via conditional diffusion modeling\n",
        "\n",
        "- Produces a pretrained U-Net used for downstream tasks\n",
        "\n",
        "- Post-ema reconstruction\n",
        "\n",
        "**2. Downstream Gene Expression Training**\n",
        "\n",
        "- Uses the pretrained U-Net (ema) from Stage 1\n",
        "\n",
        "- Trains a lightweight module combined with a foundation model (plip)\n",
        "\n",
        "- Performs evaluation and metric reporting\n",
        "\n",
        "‚ö†Ô∏è **Notes**\n",
        "\n",
        "- All hyperparameters are managed through `.yaml` configuration files\n",
        "\n",
        "- For simplicity, the 3-fold experiment used in the full evaluation is omitted in this Colab walkthrough\n",
        "\n",
        "- The notebook focuses on demonstrating the workflow and key components, not full-scale training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bwMMtPMa2nW"
      },
      "source": [
        "## üåÄ Stage I: Diffusion Pretraining\n",
        "In this notebook, we demonstrate **Stage I: Diffusion Pretraining** using a **lightweight configuration** suitable for Google Colab.\n",
        "\n",
        "To ensure fast execution, we use:\n",
        "\n",
        "- ~2 epochs (instead of the full training schedule)\n",
        "\n",
        "- Batch size = 128\n",
        "\n",
        "- Three datasets, consistent with the experimental configuration in the paper, but in a simplified demo mode\n",
        "\n",
        "\n",
        "‚ö†Ô∏è **Note:**\n",
        "The **full-scale experiments** reported in the DiffBulk paper use the settings defined in `Pretrain/train.sh`. Those settings include significantly longer training time and larger batch sizes that are not suitable *for Colab*.\n",
        "\n",
        "This demo focuses on illustrating:\n",
        "\n",
        "- How the diffusion model is trained\n",
        "\n",
        "- How gene-aware conditional denoising works\n",
        "\n",
        "- How pretrained EMA checkpoints are produced for downstream tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "WgzgInrJbOf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dbc1759-32ba-4980-ab8a-1a8327df7be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-24 14:52:41.057628: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-24 14:52:41.075357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763995961.096881   48834 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763995961.103566   48834 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763995961.120252   48834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763995961.120290   48834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763995961.120294   48834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763995961.120296   48834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:52:41.125296: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting up training config...\n",
            "\n",
            "Training config:\n",
            "{\n",
            "  \"dataset_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.GenePatchDataset\",\n",
            "    \"patch_path\": [\n",
            "      \"/content/data/HEST_bowel/train_patch.h5\",\n",
            "      \"/content/data/HEST_pancreas/train_patch.h5\",\n",
            "      \"/content/data/crunchDAO/train_patch.h5\"\n",
            "    ],\n",
            "    \"gene_path\": [\n",
            "      \"/content/data/HEST_bowel/train_gene.h5\",\n",
            "      \"/content/data/HEST_pancreas/train_gene.h5\",\n",
            "      \"/content/data/crunchDAO/train_gene.h5\"\n",
            "    ]\n",
            "  },\n",
            "  \"valid_dataset_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.GenePatchDataset\",\n",
            "    \"patch_path\": [\n",
            "      \"/content/data/HEST_bowel/valid_patch.h5\",\n",
            "      \"/content/data/HEST_pancreas/valid_patch.h5\",\n",
            "      \"/content/data/crunchDAO/valid_patch.h5\"\n",
            "    ],\n",
            "    \"gene_path\": [\n",
            "      \"/content/data/HEST_bowel/valid_gene.h5\",\n",
            "      \"/content/data/HEST_pancreas/valid_gene.h5\",\n",
            "      \"/content/data/crunchDAO/valid_gene.h5\"\n",
            "    ]\n",
            "  },\n",
            "  \"valid_batch_size\": 64,\n",
            "  \"total_nimg\": 32768,\n",
            "  \"batch_size\": 128,\n",
            "  \"network_kwargs\": {\n",
            "    \"class_name\": \"training.networks_edm2.Precond\",\n",
            "    \"model_channels\": 128,\n",
            "    \"dropout\": 0.0,\n",
            "    \"no_guidance_prob\": 0.5,\n",
            "    \"embed_dim\": 256,\n",
            "    \"num_gene_blocks\": 2,\n",
            "    \"use_fp16\": true\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.training_loop.EDM2Loss\",\n",
            "    \"P_mean\": -0.4,\n",
            "    \"P_std\": 1.0\n",
            "  },\n",
            "  \"lr_kwargs\": {\n",
            "    \"func_name\": \"training.training_loop.learning_rate_schedule\",\n",
            "    \"ref_lr\": 0.012,\n",
            "    \"ref_batches\": 70000\n",
            "  },\n",
            "  \"batch_gpu\": 16,\n",
            "  \"loss_scaling\": 1.0,\n",
            "  \"cudnn_benchmark\": true,\n",
            "  \"status_nimg\": 640,\n",
            "  \"snapshot_nimg\": 1024,\n",
            "  \"checkpoint_nimg\": 1024,\n",
            "  \"seed\": 0\n",
            "}\n",
            "\n",
            "Output directory:        /content/DiffBulk/Pretrain/outputs\n",
            "Patch Dataset path:      ('/content/data/HEST_bowel/train_patch.h5', '/content/data/HEST_pancreas/train_patch.h5', '/content/data/crunchDAO/train_patch.h5')\n",
            "Gene Dataset path:       ('/content/data/HEST_bowel/train_gene.h5', '/content/data/HEST_pancreas/train_gene.h5', '/content/data/crunchDAO/train_gene.h5')\n",
            "Valid Patch Dataset path:       ('/content/data/HEST_bowel/valid_patch.h5', '/content/data/HEST_pancreas/valid_patch.h5', '/content/data/crunchDAO/valid_patch.h5')\n",
            "Valid Gene Dataset path:       ('/content/data/HEST_bowel/valid_gene.h5', '/content/data/HEST_pancreas/valid_gene.h5', '/content/data/crunchDAO/valid_gene.h5')\n",
            "Gene Embedding dim:       256\n",
            "Number of Gene Encoder Blocks:       2\n",
            "Number of GPUs:          1\n",
            "Batch size:              128\n",
            "Mixed-precision:         True\n",
            "\n",
            "Creating output directory...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W1124 14:52:46.731572381 ProcessGroupNCCL.cpp:5068] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "Setting up training state...\n",
            "Loading dataset...\n",
            "2025-11-24 14:52:51.123100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763995971.144190   48931 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763995971.150630   48931 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763995971.166388   48931 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763995971.166416   48931 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763995971.166420   48931 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763995971.166422   48931 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:53:01.539867: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763995981.560713   49009 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763995981.567050   49009 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763995981.582636   49009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763995981.582664   49009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763995981.582667   49009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763995981.582669   49009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:53:12.026061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763995992.047165   49096 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763995992.053580   49096 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763995992.069380   49096 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763995992.069414   49096 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763995992.069417   49096 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763995992.069420   49096 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:53:21.591888: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996001.613081   49176 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996001.619537   49176 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996001.635410   49176 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996001.635439   49176 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996001.635442   49176 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996001.635444   49176 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:53:46.528753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996026.549936   49321 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996026.556365   49321 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996026.572921   49321 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996026.572946   49321 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996026.572949   49321 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996026.572952   49321 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:53:57.107347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996037.128726   49401 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996037.135193   49401 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996037.150971   49401 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996037.150998   49401 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996037.151000   49401 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996037.151003   49401 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Setting up encoder...\n",
            "Image shape: torch.Size([1, 3, 224, 224])...\n",
            "label shape: torch.Size([541])...\n",
            "Constructing network...\n",
            "\n",
            "Precond                  Parameters  Buffers  Output shape         Datatype\n",
            "---                      ---         ---      ---                  ---     \n",
            "unet.noise_emb_fourier   -           256      [16, 128]            float32 \n",
            "unet.emb_noise           65536       -        [16, 512]            float32 \n",
            "unet.emb_gene            2368000     -        [16, 512]            float32 \n",
            "unet.enc.224x224_conv    4608        -        [16, 128, 224, 224]  float16 \n",
            "unet.enc.224x224_block0  360449      -        [16, 128, 224, 224]  float16 \n",
            "unet.enc.224x224_block1  360449      -        [16, 128, 224, 224]  float16 \n",
            "unet.enc.224x224_block2  360449      -        [16, 128, 224, 224]  float16 \n",
            "unet.enc.112x112_down    360449      -        [16, 128, 112, 112]  float16 \n",
            "unet.enc.112x112_block0  1343489     -        [16, 256, 112, 112]  float16 \n",
            "unet.enc.112x112_block1  1310721     -        [16, 256, 112, 112]  float16 \n",
            "unet.enc.112x112_block2  1310721     -        [16, 256, 112, 112]  float16 \n",
            "unet.enc.56x56_down      1310721     -        [16, 256, 56, 56]    float16 \n",
            "unet.enc.56x56_block0    2949121     -        [16, 384, 56, 56]    float16 \n",
            "unet.enc.56x56_block1    2850817     -        [16, 384, 56, 56]    float16 \n",
            "unet.enc.56x56_block2    2850817     -        [16, 384, 56, 56]    float16 \n",
            "unet.enc.28x28_down      2850817     -        [16, 384, 28, 28]    float16 \n",
            "unet.enc.28x28_block0    6225921     -        [16, 512, 28, 28]    float16 \n",
            "unet.enc.28x28_block1    6029313     -        [16, 512, 28, 28]    float16 \n",
            "unet.enc.28x28_block2    6029313     -        [16, 512, 28, 28]    float16 \n",
            "unet.dec.28x28_in0       6029313     -        [16, 512, 28, 28]    float16 \n",
            "unet.dec.28x28_in1       4980737     -        [16, 512, 28, 28]    float16 \n",
            "unet.dec.28x28_block0    8912897     -        [16, 512, 28, 28]    float16 \n",
            "unet.dec.28x28_block1    8912897     -        [16, 512, 28, 28]    float16 \n",
            "unet.dec.28x28_block2    8912897     -        [16, 512, 28, 28]    float16 \n",
            "unet.dec.28x28_block3    8257537     -        [16, 512, 28, 28]    float16 \n",
            "unet.dec.56x56_up        4980737     -        [16, 512, 56, 56]    float16 \n",
            "unet.dec.56x56_block0    4964353     -        [16, 384, 56, 56]    float16 \n",
            "unet.dec.56x56_block1    4472833     -        [16, 384, 56, 56]    float16 \n",
            "unet.dec.56x56_block2    4472833     -        [16, 384, 56, 56]    float16 \n",
            "unet.dec.56x56_block3    3981313     -        [16, 384, 56, 56]    float16 \n",
            "unet.dec.112x112_up      2850817     -        [16, 384, 112, 112]  float16 \n",
            "unet.dec.112x112_block0  2359297     -        [16, 256, 112, 112]  float16 \n",
            "unet.dec.112x112_block1  2031617     -        [16, 256, 112, 112]  float16 \n",
            "unet.dec.112x112_block2  2031617     -        [16, 256, 112, 112]  float16 \n",
            "unet.dec.112x112_block3  1703937     -        [16, 256, 112, 112]  float16 \n",
            "unet.dec.224x224_up      1310721     -        [16, 256, 224, 224]  float16 \n",
            "unet.dec.224x224_block0  704513      -        [16, 128, 224, 224]  float16 \n",
            "unet.dec.224x224_block1  540673      -        [16, 128, 224, 224]  float16 \n",
            "unet.dec.224x224_block2  540673      -        [16, 128, 224, 224]  float16 \n",
            "unet.dec.224x224_block3  540673      -        [16, 128, 224, 224]  float16 \n",
            "unet.out_conv            3456        -        [16, 3, 224, 224]    float16 \n",
            "unet                     1           -        [16, 3, 224, 224]    float16 \n",
            "<top-level>              128         256      [16, 3, 224, 224]    float32 \n",
            "---                      ---         ---      ---                  ---     \n",
            "Total                    122438181   512      -                    -       \n",
            "\n",
            "Loading validation dataset...\n",
            "Training from 0 kimg to 32 kimg:\n",
            "\n",
            "Status: kimg 0.0       time 1m 24s       sec/tick 84.31    sec/kimg 0.000   maintenance 84.31   cpumem 18.67  gpumem 6.68   reserved 7.31  \n",
            "Saving network-snapshot-0000000-0.050.pkl ... done\n",
            "Saving network-snapshot-0000000-0.100.pkl ... done\n",
            "Running validation...\n",
            "2025-11-24 14:54:14.679497: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996054.700684   49518 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996054.707131   49518 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996054.723018   49518 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996054.723046   49518 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996054.723049   49518 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996054.723051   49518 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:54:22.691809: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996062.712962   49590 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996062.719354   49590 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996062.735044   49590 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996062.735073   49590 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996062.735076   49590 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996062.735079   49590 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:54:30.843670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996070.866216   49660 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996070.872739   49660 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996070.888518   49660 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996070.888551   49660 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996070.888555   49660 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996070.888558   49660 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:54:38.986405: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996079.007793   49730 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996079.014290   49730 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996079.030318   49730 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996079.030347   49730 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996079.030350   49730 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996079.030352   49730 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:55:02.742673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996102.763941   49880 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996102.770420   49880 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996102.786976   49880 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996102.787005   49880 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996102.787008   49880 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996102.787011   49880 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:55:10.631307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996110.651829   49950 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996110.658124   49950 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996110.673429   49950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996110.673458   49950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996110.673461   49950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996110.673463   49950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:55:18.503631: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996118.524810   50018 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996118.531286   50018 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996118.547155   50018 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996118.547180   50018 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996118.547183   50018 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996118.547185   50018 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:55:26.406795: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996126.427979   50088 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996126.434558   50088 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996126.450398   50088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996126.450426   50088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996126.450430   50088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996126.450432   50088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:55:43.968751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996143.990435   50212 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996143.997204   50212 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996144.013047   50212 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996144.013073   50212 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996144.013076   50212 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996144.013078   50212 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:55:52.050531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996152.071956   50284 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996152.078543   50284 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996152.094846   50284 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996152.094880   50284 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996152.094884   50284 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996152.094886   50284 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:56:00.140816: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996160.161887   50354 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996160.168271   50354 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996160.183981   50354 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996160.184008   50354 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996160.184011   50354 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996160.184014   50354 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 14:56:08.233813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763996168.254751   50424 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763996168.261102   50424 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763996168.276757   50424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996168.276788   50424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996168.276791   50424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763996168.276794   50424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Validation Loss 1: 398192.4524421594\n",
            "Validation Loss 2: 415600.23529411765\n",
            "Validation Loss 3: 331505.69976905314\n",
            "Status: kimg 0.6       time 4m 51s       sec/tick 206.24   sec/kimg 110.075 maintenance 135.80  cpumem 20.08  gpumem 30.45  reserved 32.93 \n",
            "Saving network-snapshot-0000001-0.050.pkl ... done\n",
            "Saving network-snapshot-0000001-0.100.pkl ... done\n",
            "Status: kimg 1.3       time 5m 13s       sec/tick 22.33    sec/kimg 33.412  maintenance 0.94    cpumem 20.24  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 1.9       time 5m 35s       sec/tick 21.62    sec/kimg 33.752  maintenance 0.02    cpumem 20.25  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000002-0.050.pkl ... done\n",
            "Saving network-snapshot-0000002-0.100.pkl ... done\n",
            "Status: kimg 2.6       time 5m 57s       sec/tick 22.34    sec/kimg 33.525  maintenance 0.89    cpumem 20.27  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000003-0.050.pkl ... done\n",
            "Saving network-snapshot-0000003-0.100.pkl ... done\n",
            "Status: kimg 3.2       time 6m 19s       sec/tick 22.49    sec/kimg 33.782  maintenance 0.87    cpumem 20.29  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 3.8       time 6m 41s       sec/tick 21.83    sec/kimg 34.102  maintenance 0.00    cpumem 20.30  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000004-0.050.pkl ... done\n",
            "Saving network-snapshot-0000004-0.100.pkl ... done\n",
            "Status: kimg 4.5       time 7m 04s       sec/tick 22.50    sec/kimg 33.790  maintenance 0.87    cpumem 20.32  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 5.1       time 7m 25s       sec/tick 21.56    sec/kimg 33.664  maintenance 0.02    cpumem 20.34  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000005-0.050.pkl ... done\n",
            "Saving network-snapshot-0000005-0.100.pkl ... done\n",
            "Status: kimg 5.8       time 7m 48s       sec/tick 22.31    sec/kimg 33.515  maintenance 0.86    cpumem 20.36  gpumem 30.20  reserved 32.93 \n",
            "Saving network-snapshot-0000006-0.050.pkl ... done\n",
            "Saving network-snapshot-0000006-0.100.pkl ... done\n",
            "Status: kimg 6.4       time 8m 10s       sec/tick 22.43    sec/kimg 33.691  maintenance 0.87    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 7.0       time 8m 32s       sec/tick 21.69    sec/kimg 33.891  maintenance 0.00    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000007-0.050.pkl ... done\n",
            "Saving network-snapshot-0000007-0.100.pkl ... done\n",
            "Status: kimg 7.7       time 8m 54s       sec/tick 22.58    sec/kimg 33.896  maintenance 0.88    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Training Loss at 8 kimg: 362881.328125\n",
            "Saving network-snapshot-0000008-0.050.pkl ... done\n",
            "Saving network-snapshot-0000008-0.100.pkl ... done\n",
            "Status: kimg 8.3       time 9m 17s       sec/tick 22.85    sec/kimg 33.892  maintenance 1.15    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 9.0       time 9m 39s       sec/tick 21.60    sec/kimg 33.723  maintenance 0.02    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000009-0.050.pkl ... done\n",
            "Saving network-snapshot-0000009-0.100.pkl ... done\n",
            "Status: kimg 9.6       time 10m 01s      sec/tick 22.48    sec/kimg 33.748  maintenance 0.88    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 10.2      time 10m 23s      sec/tick 21.71    sec/kimg 33.907  maintenance 0.00    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000010-0.050.pkl ... done\n",
            "Saving network-snapshot-0000010-0.100.pkl ... done\n",
            "Status: kimg 10.9      time 10m 45s      sec/tick 22.23    sec/kimg 33.379  maintenance 0.86    cpumem 20.37  gpumem 29.60  reserved 32.93 \n",
            "Saving network-snapshot-0000011-0.050.pkl ... done\n",
            "Saving network-snapshot-0000011-0.100.pkl ... done\n",
            "Status: kimg 11.5      time 11m 08s      sec/tick 22.63    sec/kimg 34.016  maintenance 0.86    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 12.2      time 11m 29s      sec/tick 21.72    sec/kimg 33.909  maintenance 0.02    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000012-0.050.pkl ... done\n",
            "Saving network-snapshot-0000012-0.100.pkl ... done\n",
            "Status: kimg 12.8      time 11m 52s      sec/tick 22.47    sec/kimg 33.724  maintenance 0.89    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000013-0.050.pkl ... done\n",
            "Saving network-snapshot-0000013-0.100.pkl ... done\n",
            "Status: kimg 13.4      time 12m 14s      sec/tick 22.44    sec/kimg 33.705  maintenance 0.87    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 14.1      time 12m 36s      sec/tick 21.72    sec/kimg 33.928  maintenance 0.00    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000014-0.050.pkl ... done\n",
            "Saving network-snapshot-0000014-0.100.pkl ... done\n",
            "Status: kimg 14.7      time 12m 59s      sec/tick 22.68    sec/kimg 34.086  maintenance 0.86    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 15.4      time 13m 20s      sec/tick 21.72    sec/kimg 33.913  maintenance 0.02    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000015-0.050.pkl ... done\n",
            "Saving network-snapshot-0000015-0.100.pkl ... done\n",
            "Status: kimg 16.0      time 13m 43s      sec/tick 22.48    sec/kimg 33.753  maintenance 0.87    cpumem 20.37  gpumem 30.21  reserved 32.93 \n",
            "Training Loss at 16 kimg: 406759.234375\n",
            "Saving network-snapshot-0000016-0.050.pkl ... done\n",
            "Saving network-snapshot-0000016-0.100.pkl ... done\n",
            "Status: kimg 16.6      time 14m 05s      sec/tick 22.48    sec/kimg 33.762  maintenance 0.87    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 17.3      time 14m 27s      sec/tick 21.50    sec/kimg 33.586  maintenance 0.00    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000017-0.050.pkl ... done\n",
            "Saving network-snapshot-0000017-0.100.pkl ... done\n",
            "Status: kimg 17.9      time 14m 49s      sec/tick 22.44    sec/kimg 33.726  maintenance 0.85    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000018-0.050.pkl ... done\n",
            "Saving network-snapshot-0000018-0.100.pkl ... done\n",
            "Status: kimg 18.6      time 15m 12s      sec/tick 22.56    sec/kimg 33.922  maintenance 0.85    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 19.2      time 15m 34s      sec/tick 21.64    sec/kimg 33.805  maintenance 0.00    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000019-0.050.pkl ... done\n",
            "Saving network-snapshot-0000019-0.100.pkl ... done\n",
            "Status: kimg 19.8      time 15m 56s      sec/tick 22.59    sec/kimg 33.942  maintenance 0.86    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 20.5      time 16m 18s      sec/tick 21.56    sec/kimg 33.668  maintenance 0.02    cpumem 20.38  gpumem 30.20  reserved 32.93 \n",
            "Saving network-snapshot-0000020-0.050.pkl ... done\n",
            "Saving network-snapshot-0000020-0.100.pkl ... done\n",
            "Status: kimg 21.1      time 16m 40s      sec/tick 22.41    sec/kimg 33.639  maintenance 0.88    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000021-0.050.pkl ... done\n",
            "Saving network-snapshot-0000021-0.100.pkl ... done\n",
            "Status: kimg 21.8      time 17m 03s      sec/tick 22.40    sec/kimg 33.641  maintenance 0.87    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 22.4      time 17m 25s      sec/tick 22.09    sec/kimg 34.501  maintenance 0.00    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000022-0.050.pkl ... done\n",
            "Saving network-snapshot-0000022-0.100.pkl ... done\n",
            "Status: kimg 23.0      time 17m 47s      sec/tick 22.58    sec/kimg 33.939  maintenance 0.86    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000023-0.050.pkl ... done\n",
            "Saving network-snapshot-0000023-0.100.pkl ... done\n",
            "Status: kimg 23.7      time 18m 10s      sec/tick 22.73    sec/kimg 34.166  maintenance 0.86    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 24.3      time 18m 32s      sec/tick 21.66    sec/kimg 33.823  maintenance 0.02    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Training Loss at 24 kimg: 324994.8125\n",
            "Saving network-snapshot-0000024-0.050.pkl ... done\n",
            "Saving network-snapshot-0000024-0.100.pkl ... done\n",
            "Status: kimg 25.0      time 18m 54s      sec/tick 22.76    sec/kimg 34.197  maintenance 0.87    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 25.6      time 19m 16s      sec/tick 21.82    sec/kimg 34.065  maintenance 0.02    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000025-0.050.pkl ... done\n",
            "Saving network-snapshot-0000025-0.100.pkl ... done\n",
            "Status: kimg 26.2      time 19m 39s      sec/tick 22.65    sec/kimg 34.044  maintenance 0.86    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000026-0.050.pkl ... done\n",
            "Saving network-snapshot-0000026-0.100.pkl ... done\n",
            "Status: kimg 26.9      time 20m 02s      sec/tick 22.77    sec/kimg 34.198  maintenance 0.88    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 27.5      time 20m 23s      sec/tick 21.71    sec/kimg 33.900  maintenance 0.02    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000027-0.050.pkl ... done\n",
            "Saving network-snapshot-0000027-0.100.pkl ... done\n",
            "Status: kimg 28.2      time 20m 46s      sec/tick 22.86    sec/kimg 34.360  maintenance 0.87    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000028-0.050.pkl ... done\n",
            "Saving network-snapshot-0000028-0.100.pkl ... done\n",
            "Status: kimg 28.8      time 21m 09s      sec/tick 22.43    sec/kimg 33.683  maintenance 0.87    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 29.4      time 21m 30s      sec/tick 21.53    sec/kimg 33.634  maintenance 0.00    cpumem 20.39  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000029-0.050.pkl ... done\n",
            "Saving network-snapshot-0000029-0.100.pkl ... done\n",
            "Status: kimg 30.1      time 21m 52s      sec/tick 22.38    sec/kimg 33.629  maintenance 0.86    cpumem 20.39  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 30.7      time 22m 14s      sec/tick 21.86    sec/kimg 34.142  maintenance 0.00    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000030-0.050.pkl ... done\n",
            "Saving network-snapshot-0000030-0.100.pkl ... done\n",
            "Status: kimg 31.4      time 22m 37s      sec/tick 22.50    sec/kimg 33.813  maintenance 0.86    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Saving network-snapshot-0000031-0.050.pkl ... done\n",
            "Saving network-snapshot-0000031-0.100.pkl ... done\n",
            "Status: kimg 32.0      time 22m 59s      sec/tick 22.62    sec/kimg 33.993  maintenance 0.87    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Status: kimg 32.6      time 23m 21s      sec/tick 21.65    sec/kimg 33.824  maintenance 0.00    cpumem 20.38  gpumem 30.21  reserved 32.93 \n",
            "Training Loss at 32 kimg: 265542.083984375\n",
            "Status: kimg 32.8      time 23m 25s      sec/tick 4.29     sec/kimg 33.515  maintenance 0.00    cpumem 20.38  gpumem 29.61  reserved 32.93 \n",
            "Saving network-snapshot-0000032-0.050.pkl ... done\n",
            "Saving network-snapshot-0000032-0.100.pkl ... done\n",
            "[rank0]:[W1124 15:16:18.517382617 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "!torchrun --standalone --nproc_per_node=1 /content/DiffBulk/Pretrain/train.py \\\n",
        "    --outdir=\"/content/DiffBulk/Pretrain/outputs\" \\\n",
        "    --patch_path=\"/content/data/HEST_bowel/train_patch.h5\" \\\n",
        "    --patch_path=\"/content/data/HEST_pancreas/train_patch.h5\" \\\n",
        "    --patch_path=\"/content/data/crunchDAO/train_patch.h5\" \\\n",
        "    --gene_path=\"/content/data/HEST_bowel/train_gene.h5\" \\\n",
        "    --gene_path=\"/content/data/HEST_pancreas/train_gene.h5\" \\\n",
        "    --gene_path=\"/content/data/crunchDAO/train_gene.h5\" \\\n",
        "    --valid_patch_path=\"/content/data/HEST_bowel/valid_patch.h5\" \\\n",
        "    --valid_patch_path=\"/content/data/HEST_pancreas/valid_patch.h5\" \\\n",
        "    --valid_patch_path=\"/content/data/crunchDAO/valid_patch.h5\" \\\n",
        "    --valid_gene_path=\"/content/data/HEST_bowel/valid_gene.h5\" \\\n",
        "    --valid_gene_path=\"/content/data/HEST_pancreas/valid_gene.h5\" \\\n",
        "    --valid_gene_path=\"/content/data/crunchDAO/valid_gene.h5\" \\\n",
        "    --embed_dim=256 \\\n",
        "    --num_gene_blocks=2 \\\n",
        "    --preset=\"gene-img224-xs\" \\\n",
        "    --batch_size=128 \\\n",
        "    --duration=$((1<<15)) \\\n",
        "    --status=$((5<<7)) \\\n",
        "    --snapshot=$((1<<10)) \\\n",
        "    --checkpoint=$((1<<10)) \\\n",
        "    --batch-gpu=16 \\\n",
        "    --valid_interval_nimg=$((1<<10)) \\\n",
        "    --valid_batch_size=64 \\\n",
        "    --p=0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìò Post-EMA Reconstruction\n",
        "\n",
        "Extract EMA-smoothed checkpoints for downstream training."
      ],
      "metadata": {
        "id": "Rz7oPgD8RF5_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "i9nL4fzSb6FE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308ffa6d-a8d5-4921-cc3b-329e28053275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 64 input pickles...\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000001-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000001-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000002-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000002-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000003-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000003-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000004-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000004-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000005-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000005-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000006-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000006-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000007-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000007-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000008-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000008-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000009-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000009-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000010-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000010-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000011-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000011-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000012-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000012-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000013-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000013-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000014-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000014-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000015-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000015-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000016-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000016-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000017-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000017-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000018-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000018-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000019-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000019-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000020-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000020-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000021-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000021-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000022-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000022-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000023-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000023-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000024-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000024-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000025-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000025-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000026-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000026-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000027-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000027-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000028-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000028-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000029-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000029-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000030-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000030-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000031-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000031-0.100.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000032-0.050.pkl\n",
            "    /content/DiffBulk/Pretrain/outputs/network-snapshot-0000032-0.100.pkl\n",
            "Reconstructing 3 output pickles in 1 batches...\n",
            "    batch 0: /content/DiffBulk/Pretrain/ema/phema-0000032-0.100.pkl\n",
            "    batch 0: /content/DiffBulk/Pretrain/ema/phema-0000032-0.150.pkl\n",
            "    batch 0: /content/DiffBulk/Pretrain/ema/phema-0000032-0.200.pkl\n",
            "\r  0% 0/64 [00:00<?, ?step/s]2025-11-24 15:16:41.003453: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-24 15:16:41.021936: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763997401.043416   55533 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763997401.049931   55533 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763997401.066616   55533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763997401.066643   55533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763997401.066646   55533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763997401.066648   55533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 15:16:41.071519: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "100% 64/64 [00:29<00:00,  2.17step/s]\n"
          ]
        }
      ],
      "source": [
        "# Reconstruct a new EMA profile with std=0.150\n",
        "!python /content/DiffBulk/Pretrain/reconstruct_phema.py --indir=\"/content/DiffBulk/Pretrain/outputs\" \\\n",
        "    --outdir=\"/content/DiffBulk/Pretrain/ema\" \\\n",
        "    --outstd=0.10,0.15,0.20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--aldMsHOZkP"
      },
      "source": [
        "## üî¨ Stage II: Downstream Gene Expression Training\n",
        "\n",
        "This stage fine-tunes a fusion network that integrates:\n",
        "\n",
        "- Diffusion-pretrained gene-aware image features, and\n",
        "\n",
        "- A pathology foundation model (e.g., PLIP)\n",
        "\n",
        "**1. Configure `Downstream/config.yaml`**\n",
        "\n",
        "Key arguments:\n",
        "\n",
        "  - `diffusion_path`: Path to the post-EMA reconstructed checkpoint from Stage I (usually under `Pretrain/ema/`).\n",
        "  - `noise_label`: A small amount of Gaussian noise added to image patches during training, improving robustness.\n",
        "  - `out_dim`: Total number of gene targets being predicted.\n",
        "  - `fusion_method`: Specifies how the diffusion branch interacts with the foundation model branch. For example, `\"gated_residual\"` adaptively fuses two feature streams.\n",
        "  - `c/c_learnable`: Weight controlling the contribution of the diffusion features. When `c_learnable=True`, the model learns this weight automatically.\n",
        "\n",
        "## ‚úÖ Copy the following YAML to `Downstream/config.yaml`:\n",
        "\n",
        "```yaml\n",
        "# data\n",
        "train_patch_file: \"/content/data/HEST_bowel/train_patch.h5\"\n",
        "train_gene_file: \"/content/data/HEST_bowel/train_gene.h5\"\n",
        "valid_patch_file: \"/content/data/HEST_bowel/valid_patch.h5\"\n",
        "valid_gene_file: \"/content/data/HEST_bowel/valid_gene.h5\"\n",
        "\n",
        "# pretrained model\n",
        "diffusion_path: \"/content/DiffBulk/Pretrain/ema/phema-0000032-0.100.pkl\"\n",
        "\n",
        "# hyper-parameters\n",
        "noise_label: 0.01\n",
        "out_dim: 541\n",
        "fusion_method: 'gated_residual'\n",
        "c: 1.0\n",
        "c_learnable: True\n",
        "\n",
        "# training\n",
        "epochs: 4\n",
        "device: cuda\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "weight_decay: 0.00001\n",
        "\n",
        "# logging\n",
        "tensorboard_dir: \"./tensorboard\"\n",
        "checkpoint_dir: \"./ckpts\"\n",
        "log_interval: 2\n",
        "valid_interval: 2\n",
        "start_valid: 0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVxGkqC4dPui"
      },
      "source": [
        "**2. Start training**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change working dir\n",
        "import os\n",
        "os.chdir(\"/content/DiffBulk/Downstream\")\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-o3VfhQjQX8",
        "outputId": "b971ac74-98e6-4b06-ba16-24de5977a42f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DiffBulk/Downstream\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxJcLMKMYeDt",
        "outputId": "de24d9e3-6995-41a8-8524-355986e5a32b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-24 15:48:34.284693: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-24 15:48:34.302475: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763999314.324044   63807 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763999314.330673   63807 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763999314.347290   63807 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763999314.347318   63807 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763999314.347321   63807 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763999314.347324   63807 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 15:48:34.352185: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loaded configuration:\n",
            "train_patch_file: /content/data/HEST_bowel/train_patch.h5\n",
            "train_gene_file: /content/data/HEST_bowel/train_gene.h5\n",
            "valid_patch_file: /content/data/HEST_bowel/valid_patch.h5\n",
            "valid_gene_file: /content/data/HEST_bowel/valid_gene.h5\n",
            "diffusion_path: /content/DiffBulk/Pretrain/ema/phema-0000032-0.100.pkl\n",
            "noise_label: 0.01\n",
            "out_dim: 541\n",
            "fusion_method: gated_residual\n",
            "c: 1.0\n",
            "c_learnable: True\n",
            "epochs: 4\n",
            "device: cuda\n",
            "batch_size: 32\n",
            "lr: 0.0001\n",
            "weight_decay: 1e-05\n",
            "tensorboard_dir: ./tensorboard\n",
            "checkpoint_dir: ./ckpts\n",
            "log_interval: 2\n",
            "valid_interval: 2\n",
            "start_valid: 0\n",
            "==================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Epoch [1/4], Step [2/219], Loss: 3.4538\n",
            "Epoch [1/4], Step [4/219], Loss: 3.9396\n",
            "Epoch [1/4], Step [6/219], Loss: 3.5775\n",
            "Epoch [1/4], Step [8/219], Loss: 2.8903\n",
            "Epoch [1/4], Step [10/219], Loss: 3.5675\n",
            "Epoch [1/4], Step [12/219], Loss: 3.2912\n",
            "Epoch [1/4], Step [14/219], Loss: 2.9987\n",
            "Epoch [1/4], Step [16/219], Loss: 2.6316\n",
            "Epoch [1/4], Step [18/219], Loss: 2.5226\n",
            "Epoch [1/4], Step [20/219], Loss: 2.9496\n",
            "Epoch [1/4], Step [22/219], Loss: 2.7525\n",
            "Epoch [1/4], Step [24/219], Loss: 2.1632\n",
            "Epoch [1/4], Step [26/219], Loss: 1.9825\n",
            "Epoch [1/4], Step [28/219], Loss: 2.4356\n",
            "Epoch [1/4], Step [30/219], Loss: 2.0388\n",
            "Epoch [1/4], Step [32/219], Loss: 1.6861\n",
            "Epoch [1/4], Step [34/219], Loss: 1.7929\n",
            "Epoch [1/4], Step [36/219], Loss: 1.6315\n",
            "Epoch [1/4], Step [38/219], Loss: 1.5168\n",
            "Epoch [1/4], Step [40/219], Loss: 1.3856\n",
            "Epoch [1/4], Step [42/219], Loss: 1.5731\n",
            "Epoch [1/4], Step [44/219], Loss: 1.3289\n",
            "Epoch [1/4], Step [46/219], Loss: 1.5554\n",
            "Epoch [1/4], Step [48/219], Loss: 1.4584\n",
            "Epoch [1/4], Step [50/219], Loss: 1.2876\n",
            "Epoch [1/4], Step [52/219], Loss: 1.3894\n",
            "Epoch [1/4], Step [54/219], Loss: 1.3540\n",
            "Epoch [1/4], Step [56/219], Loss: 1.2086\n",
            "Epoch [1/4], Step [58/219], Loss: 1.2320\n",
            "Epoch [1/4], Step [60/219], Loss: 1.2326\n",
            "Epoch [1/4], Step [62/219], Loss: 1.4628\n",
            "Epoch [1/4], Step [64/219], Loss: 1.4051\n",
            "Epoch [1/4], Step [66/219], Loss: 1.3049\n",
            "Epoch [1/4], Step [68/219], Loss: 1.2708\n",
            "Epoch [1/4], Step [70/219], Loss: 1.2148\n",
            "Epoch [1/4], Step [72/219], Loss: 1.0960\n",
            "Epoch [1/4], Step [74/219], Loss: 1.4286\n",
            "Epoch [1/4], Step [76/219], Loss: 1.3267\n",
            "Epoch [1/4], Step [78/219], Loss: 1.3906\n",
            "Epoch [1/4], Step [80/219], Loss: 1.2236\n",
            "Epoch [1/4], Step [82/219], Loss: 1.2764\n",
            "Epoch [1/4], Step [84/219], Loss: 1.2693\n",
            "Epoch [1/4], Step [86/219], Loss: 1.1595\n",
            "Epoch [1/4], Step [88/219], Loss: 1.1858\n",
            "Epoch [1/4], Step [90/219], Loss: 1.1691\n",
            "Epoch [1/4], Step [92/219], Loss: 1.2247\n",
            "Epoch [1/4], Step [94/219], Loss: 1.1013\n",
            "Epoch [1/4], Step [96/219], Loss: 1.2276\n",
            "Epoch [1/4], Step [98/219], Loss: 1.1003\n",
            "Epoch [1/4], Step [100/219], Loss: 1.1515\n",
            "Epoch [1/4], Step [102/219], Loss: 1.0602\n",
            "Epoch [1/4], Step [104/219], Loss: 1.2253\n",
            "Epoch [1/4], Step [106/219], Loss: 1.0639\n",
            "Epoch [1/4], Step [108/219], Loss: 1.0835\n",
            "Epoch [1/4], Step [110/219], Loss: 1.0396\n",
            "Epoch [1/4], Step [112/219], Loss: 1.0874\n",
            "Epoch [1/4], Step [114/219], Loss: 0.9043\n",
            "Epoch [1/4], Step [116/219], Loss: 0.9052\n",
            "Epoch [1/4], Step [118/219], Loss: 0.9710\n",
            "Epoch [1/4], Step [120/219], Loss: 0.9880\n",
            "Epoch [1/4], Step [122/219], Loss: 0.9408\n",
            "Epoch [1/4], Step [124/219], Loss: 0.9815\n",
            "Epoch [1/4], Step [126/219], Loss: 0.8979\n",
            "Epoch [1/4], Step [128/219], Loss: 0.8197\n",
            "Epoch [1/4], Step [130/219], Loss: 0.8889\n",
            "Epoch [1/4], Step [132/219], Loss: 0.8850\n",
            "Epoch [1/4], Step [134/219], Loss: 0.9231\n",
            "Epoch [1/4], Step [136/219], Loss: 0.7632\n",
            "Epoch [1/4], Step [138/219], Loss: 0.7692\n",
            "Epoch [1/4], Step [140/219], Loss: 0.9153\n",
            "Epoch [1/4], Step [142/219], Loss: 0.6480\n",
            "Epoch [1/4], Step [144/219], Loss: 0.7799\n",
            "Epoch [1/4], Step [146/219], Loss: 0.8087\n",
            "Epoch [1/4], Step [148/219], Loss: 0.8779\n",
            "Epoch [1/4], Step [150/219], Loss: 0.7503\n",
            "Epoch [1/4], Step [152/219], Loss: 0.8841\n",
            "Epoch [1/4], Step [154/219], Loss: 0.7412\n",
            "Epoch [1/4], Step [156/219], Loss: 0.7480\n",
            "Epoch [1/4], Step [158/219], Loss: 0.6890\n",
            "Epoch [1/4], Step [160/219], Loss: 0.7932\n",
            "Epoch [1/4], Step [162/219], Loss: 0.8686\n",
            "Epoch [1/4], Step [164/219], Loss: 0.6933\n",
            "Epoch [1/4], Step [166/219], Loss: 0.7376\n",
            "Epoch [1/4], Step [168/219], Loss: 0.8190\n",
            "Epoch [1/4], Step [170/219], Loss: 0.6993\n",
            "Epoch [1/4], Step [172/219], Loss: 0.6841\n",
            "Epoch [1/4], Step [174/219], Loss: 0.6996\n",
            "Epoch [1/4], Step [176/219], Loss: 0.6414\n",
            "Epoch [1/4], Step [178/219], Loss: 0.6063\n",
            "Epoch [1/4], Step [180/219], Loss: 0.7382\n",
            "Epoch [1/4], Step [182/219], Loss: 0.8062\n",
            "Epoch [1/4], Step [184/219], Loss: 0.5755\n",
            "Epoch [1/4], Step [186/219], Loss: 0.5797\n",
            "Epoch [1/4], Step [188/219], Loss: 0.7272\n",
            "Epoch [1/4], Step [190/219], Loss: 0.6265\n",
            "Epoch [1/4], Step [192/219], Loss: 0.5743\n",
            "Epoch [1/4], Step [194/219], Loss: 0.5877\n",
            "Epoch [1/4], Step [196/219], Loss: 0.5832\n",
            "Epoch [1/4], Step [198/219], Loss: 0.7040\n",
            "Epoch [1/4], Step [200/219], Loss: 0.6683\n",
            "Epoch [1/4], Step [202/219], Loss: 0.7667\n",
            "Epoch [1/4], Step [204/219], Loss: 0.6391\n",
            "Epoch [1/4], Step [206/219], Loss: 0.5797\n",
            "Epoch [1/4], Step [208/219], Loss: 0.7122\n",
            "Epoch [1/4], Step [210/219], Loss: 0.7157\n",
            "Epoch [1/4], Step [212/219], Loss: 0.6426\n",
            "Epoch [1/4], Step [214/219], Loss: 0.6125\n",
            "Epoch [1/4], Step [216/219], Loss: 0.6556\n",
            "Epoch [1/4], Step [218/219], Loss: 0.5922\n",
            "Epoch [1/4], Average Loss: 1.2543\n",
            "Epoch [1/4], Validation MSE: 0.6023, MAE: 0.5575, Pearson: 0.8609\n",
            "Checkpoint saved at ./ckpts/checkpoint_best.pth!\n",
            "Epoch [2/4], Step [2/219], Loss: 0.6498\n",
            "Epoch [2/4], Step [4/219], Loss: 0.6158\n",
            "Epoch [2/4], Step [6/219], Loss: 0.5703\n",
            "Epoch [2/4], Step [8/219], Loss: 0.6553\n",
            "Epoch [2/4], Step [10/219], Loss: 0.5236\n",
            "Epoch [2/4], Step [12/219], Loss: 0.5846\n",
            "Epoch [2/4], Step [14/219], Loss: 0.5606\n",
            "Epoch [2/4], Step [16/219], Loss: 0.6550\n",
            "Epoch [2/4], Step [18/219], Loss: 0.5750\n",
            "Epoch [2/4], Step [20/219], Loss: 0.5645\n",
            "Epoch [2/4], Step [22/219], Loss: 0.7301\n",
            "Epoch [2/4], Step [24/219], Loss: 0.6074\n",
            "Epoch [2/4], Step [26/219], Loss: 0.6671\n",
            "Epoch [2/4], Step [28/219], Loss: 0.6061\n",
            "Epoch [2/4], Step [30/219], Loss: 0.6050\n",
            "Epoch [2/4], Step [32/219], Loss: 0.5211\n",
            "Epoch [2/4], Step [34/219], Loss: 0.7202\n",
            "Epoch [2/4], Step [36/219], Loss: 0.5347\n",
            "Epoch [2/4], Step [38/219], Loss: 0.5523\n",
            "Epoch [2/4], Step [40/219], Loss: 0.5445\n",
            "Epoch [2/4], Step [42/219], Loss: 0.5249\n",
            "Epoch [2/4], Step [44/219], Loss: 0.4980\n",
            "Epoch [2/4], Step [46/219], Loss: 0.5156\n",
            "Epoch [2/4], Step [48/219], Loss: 0.6084\n",
            "Epoch [2/4], Step [50/219], Loss: 0.6190\n",
            "Epoch [2/4], Step [52/219], Loss: 0.5866\n",
            "Epoch [2/4], Step [54/219], Loss: 0.5410\n",
            "Epoch [2/4], Step [56/219], Loss: 0.4915\n",
            "Epoch [2/4], Step [58/219], Loss: 0.5254\n",
            "Epoch [2/4], Step [60/219], Loss: 0.5086\n",
            "Epoch [2/4], Step [62/219], Loss: 0.5868\n",
            "Epoch [2/4], Step [64/219], Loss: 0.6281\n",
            "Epoch [2/4], Step [66/219], Loss: 0.5224\n",
            "Epoch [2/4], Step [68/219], Loss: 0.4525\n",
            "Epoch [2/4], Step [70/219], Loss: 0.6019\n",
            "Epoch [2/4], Step [72/219], Loss: 0.5830\n",
            "Epoch [2/4], Step [74/219], Loss: 0.4857\n",
            "Epoch [2/4], Step [76/219], Loss: 0.5756\n",
            "Epoch [2/4], Step [78/219], Loss: 0.6188\n",
            "Epoch [2/4], Step [80/219], Loss: 0.5460\n",
            "Epoch [2/4], Step [82/219], Loss: 0.5337\n",
            "Epoch [2/4], Step [84/219], Loss: 0.4708\n",
            "Epoch [2/4], Step [86/219], Loss: 0.4351\n",
            "Epoch [2/4], Step [88/219], Loss: 0.5285\n",
            "Epoch [2/4], Step [90/219], Loss: 0.4504\n",
            "Epoch [2/4], Step [92/219], Loss: 0.6093\n",
            "Epoch [2/4], Step [94/219], Loss: 0.5495\n",
            "Epoch [2/4], Step [96/219], Loss: 0.5827\n",
            "Epoch [2/4], Step [98/219], Loss: 0.5714\n",
            "Epoch [2/4], Step [100/219], Loss: 0.4426\n",
            "Epoch [2/4], Step [102/219], Loss: 0.4820\n",
            "Epoch [2/4], Step [104/219], Loss: 0.4516\n",
            "Epoch [2/4], Step [106/219], Loss: 0.4891\n",
            "Epoch [2/4], Step [108/219], Loss: 0.4997\n",
            "Epoch [2/4], Step [110/219], Loss: 0.5642\n",
            "Epoch [2/4], Step [112/219], Loss: 0.5133\n",
            "Epoch [2/4], Step [114/219], Loss: 0.5589\n",
            "Epoch [2/4], Step [116/219], Loss: 0.4195\n",
            "Epoch [2/4], Step [118/219], Loss: 0.5968\n",
            "Epoch [2/4], Step [120/219], Loss: 0.5116\n",
            "Epoch [2/4], Step [122/219], Loss: 0.5059\n",
            "Epoch [2/4], Step [124/219], Loss: 0.5043\n",
            "Epoch [2/4], Step [126/219], Loss: 0.5413\n",
            "Epoch [2/4], Step [128/219], Loss: 0.5151\n",
            "Epoch [2/4], Step [130/219], Loss: 0.5235\n",
            "Epoch [2/4], Step [132/219], Loss: 0.5222\n",
            "Epoch [2/4], Step [134/219], Loss: 0.4479\n",
            "Epoch [2/4], Step [136/219], Loss: 0.5180\n",
            "Epoch [2/4], Step [138/219], Loss: 0.5093\n",
            "Epoch [2/4], Step [140/219], Loss: 0.6011\n",
            "Epoch [2/4], Step [142/219], Loss: 0.5920\n",
            "Epoch [2/4], Step [144/219], Loss: 0.4380\n",
            "Epoch [2/4], Step [146/219], Loss: 0.4040\n",
            "Epoch [2/4], Step [148/219], Loss: 0.4942\n",
            "Epoch [2/4], Step [150/219], Loss: 0.5956\n",
            "Epoch [2/4], Step [152/219], Loss: 0.5009\n",
            "Epoch [2/4], Step [154/219], Loss: 0.5567\n",
            "Epoch [2/4], Step [156/219], Loss: 0.4085\n",
            "Epoch [2/4], Step [158/219], Loss: 0.4904\n",
            "Epoch [2/4], Step [160/219], Loss: 0.4968\n",
            "Epoch [2/4], Step [162/219], Loss: 0.5313\n",
            "Epoch [2/4], Step [164/219], Loss: 0.4291\n",
            "Epoch [2/4], Step [166/219], Loss: 0.4612\n",
            "Epoch [2/4], Step [168/219], Loss: 0.5305\n",
            "Epoch [2/4], Step [170/219], Loss: 0.4976\n",
            "Epoch [2/4], Step [172/219], Loss: 0.5503\n",
            "Epoch [2/4], Step [174/219], Loss: 0.4429\n",
            "Epoch [2/4], Step [176/219], Loss: 0.4853\n",
            "Epoch [2/4], Step [178/219], Loss: 0.5530\n",
            "Epoch [2/4], Step [180/219], Loss: 0.4331\n",
            "Epoch [2/4], Step [182/219], Loss: 0.5048\n",
            "Epoch [2/4], Step [184/219], Loss: 0.5759\n",
            "Epoch [2/4], Step [186/219], Loss: 0.4600\n",
            "Epoch [2/4], Step [188/219], Loss: 0.5554\n",
            "Epoch [2/4], Step [190/219], Loss: 0.4955\n",
            "Epoch [2/4], Step [192/219], Loss: 0.4838\n",
            "Epoch [2/4], Step [194/219], Loss: 0.4701\n",
            "Epoch [2/4], Step [196/219], Loss: 0.4519\n",
            "Epoch [2/4], Step [198/219], Loss: 0.4032\n",
            "Epoch [2/4], Step [200/219], Loss: 0.4436\n",
            "Epoch [2/4], Step [202/219], Loss: 0.4812\n",
            "Epoch [2/4], Step [204/219], Loss: 0.4889\n",
            "Epoch [2/4], Step [206/219], Loss: 0.5058\n",
            "Epoch [2/4], Step [208/219], Loss: 0.5405\n",
            "Epoch [2/4], Step [210/219], Loss: 0.4611\n",
            "Epoch [2/4], Step [212/219], Loss: 0.4444\n",
            "Epoch [2/4], Step [214/219], Loss: 0.4615\n",
            "Epoch [2/4], Step [216/219], Loss: 0.4285\n",
            "Epoch [2/4], Step [218/219], Loss: 0.3969\n",
            "Epoch [2/4], Average Loss: 0.5300\n",
            "Epoch [3/4], Step [2/219], Loss: 0.4919\n",
            "Epoch [3/4], Step [4/219], Loss: 0.5024\n",
            "Epoch [3/4], Step [6/219], Loss: 0.6594\n",
            "Epoch [3/4], Step [8/219], Loss: 0.5776\n",
            "Epoch [3/4], Step [10/219], Loss: 0.4720\n",
            "Epoch [3/4], Step [12/219], Loss: 0.4553\n",
            "Epoch [3/4], Step [14/219], Loss: 0.5084\n",
            "Epoch [3/4], Step [16/219], Loss: 0.5165\n",
            "Epoch [3/4], Step [18/219], Loss: 0.4840\n",
            "Epoch [3/4], Step [20/219], Loss: 0.5810\n",
            "Epoch [3/4], Step [22/219], Loss: 0.4681\n",
            "Epoch [3/4], Step [24/219], Loss: 0.5167\n",
            "Epoch [3/4], Step [26/219], Loss: 0.4943\n",
            "Epoch [3/4], Step [28/219], Loss: 0.4504\n",
            "Epoch [3/4], Step [30/219], Loss: 0.4763\n",
            "Epoch [3/4], Step [32/219], Loss: 0.4280\n",
            "Epoch [3/4], Step [34/219], Loss: 0.4887\n",
            "Epoch [3/4], Step [36/219], Loss: 0.5143\n",
            "Epoch [3/4], Step [38/219], Loss: 0.4755\n",
            "Epoch [3/4], Step [40/219], Loss: 0.4581\n",
            "Epoch [3/4], Step [42/219], Loss: 0.5036\n",
            "Epoch [3/4], Step [44/219], Loss: 0.4362\n",
            "Epoch [3/4], Step [46/219], Loss: 0.4412\n",
            "Epoch [3/4], Step [48/219], Loss: 0.4269\n",
            "Epoch [3/4], Step [50/219], Loss: 0.4206\n",
            "Epoch [3/4], Step [52/219], Loss: 0.4321\n",
            "Epoch [3/4], Step [54/219], Loss: 0.4674\n",
            "Epoch [3/4], Step [56/219], Loss: 0.4778\n",
            "Epoch [3/4], Step [58/219], Loss: 0.4870\n",
            "Epoch [3/4], Step [60/219], Loss: 0.4233\n",
            "Epoch [3/4], Step [62/219], Loss: 0.4441\n",
            "Epoch [3/4], Step [64/219], Loss: 0.4486\n",
            "Epoch [3/4], Step [66/219], Loss: 0.4263\n",
            "Epoch [3/4], Step [68/219], Loss: 0.4819\n",
            "Epoch [3/4], Step [70/219], Loss: 0.4368\n",
            "Epoch [3/4], Step [72/219], Loss: 0.4479\n",
            "Epoch [3/4], Step [74/219], Loss: 0.4408\n",
            "Epoch [3/4], Step [76/219], Loss: 0.5060\n",
            "Epoch [3/4], Step [78/219], Loss: 0.4483\n",
            "Epoch [3/4], Step [80/219], Loss: 0.5193\n",
            "Epoch [3/4], Step [82/219], Loss: 0.5146\n",
            "Epoch [3/4], Step [84/219], Loss: 0.4822\n",
            "Epoch [3/4], Step [86/219], Loss: 0.4426\n",
            "Epoch [3/4], Step [88/219], Loss: 0.4537\n",
            "Epoch [3/4], Step [90/219], Loss: 0.4078\n",
            "Epoch [3/4], Step [92/219], Loss: 0.4121\n",
            "Epoch [3/4], Step [94/219], Loss: 0.4816\n",
            "Epoch [3/4], Step [96/219], Loss: 0.4859\n",
            "Epoch [3/4], Step [98/219], Loss: 0.5130\n",
            "Epoch [3/4], Step [100/219], Loss: 0.4114\n",
            "Epoch [3/4], Step [102/219], Loss: 0.3956\n",
            "Epoch [3/4], Step [104/219], Loss: 0.4940\n",
            "Epoch [3/4], Step [106/219], Loss: 0.4795\n",
            "Epoch [3/4], Step [108/219], Loss: 0.4555\n",
            "Epoch [3/4], Step [110/219], Loss: 0.5443\n",
            "Epoch [3/4], Step [112/219], Loss: 0.4178\n",
            "Epoch [3/4], Step [114/219], Loss: 0.4423\n",
            "Epoch [3/4], Step [116/219], Loss: 0.5328\n",
            "Epoch [3/4], Step [118/219], Loss: 0.4512\n",
            "Epoch [3/4], Step [120/219], Loss: 0.4069\n",
            "Epoch [3/4], Step [122/219], Loss: 0.4778\n",
            "Epoch [3/4], Step [124/219], Loss: 0.4430\n",
            "Epoch [3/4], Step [126/219], Loss: 0.4448\n",
            "Epoch [3/4], Step [128/219], Loss: 0.4132\n",
            "Epoch [3/4], Step [130/219], Loss: 0.4521\n",
            "Epoch [3/4], Step [132/219], Loss: 0.4598\n",
            "Epoch [3/4], Step [134/219], Loss: 0.4297\n",
            "Epoch [3/4], Step [136/219], Loss: 0.4486\n",
            "Epoch [3/4], Step [138/219], Loss: 0.4472\n",
            "Epoch [3/4], Step [140/219], Loss: 0.4548\n",
            "Epoch [3/4], Step [142/219], Loss: 0.4095\n",
            "Epoch [3/4], Step [144/219], Loss: 0.4684\n",
            "Epoch [3/4], Step [146/219], Loss: 0.4460\n",
            "Epoch [3/4], Step [148/219], Loss: 0.4614\n",
            "Epoch [3/4], Step [150/219], Loss: 0.4429\n",
            "Epoch [3/4], Step [152/219], Loss: 0.5787\n",
            "Epoch [3/4], Step [154/219], Loss: 0.4117\n",
            "Epoch [3/4], Step [156/219], Loss: 0.4418\n",
            "Epoch [3/4], Step [158/219], Loss: 0.4160\n",
            "Epoch [3/4], Step [160/219], Loss: 0.4685\n",
            "Epoch [3/4], Step [162/219], Loss: 0.4693\n",
            "Epoch [3/4], Step [164/219], Loss: 0.3319\n",
            "Epoch [3/4], Step [166/219], Loss: 0.4530\n",
            "Epoch [3/4], Step [168/219], Loss: 0.5159\n",
            "Epoch [3/4], Step [170/219], Loss: 0.4528\n",
            "Epoch [3/4], Step [172/219], Loss: 0.4100\n",
            "Epoch [3/4], Step [174/219], Loss: 0.4937\n",
            "Epoch [3/4], Step [176/219], Loss: 0.4442\n",
            "Epoch [3/4], Step [178/219], Loss: 0.4416\n",
            "Epoch [3/4], Step [180/219], Loss: 0.4105\n",
            "Epoch [3/4], Step [182/219], Loss: 0.4674\n",
            "Epoch [3/4], Step [184/219], Loss: 0.4714\n",
            "Epoch [3/4], Step [186/219], Loss: 0.4157\n",
            "Epoch [3/4], Step [188/219], Loss: 0.4673\n",
            "Epoch [3/4], Step [190/219], Loss: 0.4494\n",
            "Epoch [3/4], Step [192/219], Loss: 0.4674\n",
            "Epoch [3/4], Step [194/219], Loss: 0.4813\n",
            "Epoch [3/4], Step [196/219], Loss: 0.4820\n",
            "Epoch [3/4], Step [198/219], Loss: 0.5930\n",
            "Epoch [3/4], Step [200/219], Loss: 0.4105\n",
            "Epoch [3/4], Step [202/219], Loss: 0.4548\n",
            "Epoch [3/4], Step [204/219], Loss: 0.4768\n",
            "Epoch [3/4], Step [206/219], Loss: 0.4713\n",
            "Epoch [3/4], Step [208/219], Loss: 0.4821\n",
            "Epoch [3/4], Step [210/219], Loss: 0.4101\n",
            "Epoch [3/4], Step [212/219], Loss: 0.4778\n",
            "Epoch [3/4], Step [214/219], Loss: 0.4526\n",
            "Epoch [3/4], Step [216/219], Loss: 0.4249\n",
            "Epoch [3/4], Step [218/219], Loss: 0.4673\n",
            "Epoch [3/4], Average Loss: 0.4653\n",
            "Epoch [3/4], Validation MSE: 0.4337, MAE: 0.4699, Pearson: 0.9009\n",
            "Checkpoint saved at ./ckpts/checkpoint_best.pth!\n",
            "Epoch [4/4], Step [2/219], Loss: 0.4525\n",
            "Epoch [4/4], Step [4/219], Loss: 0.4085\n",
            "Epoch [4/4], Step [6/219], Loss: 0.4138\n",
            "Epoch [4/4], Step [8/219], Loss: 0.4098\n",
            "Epoch [4/4], Step [10/219], Loss: 0.4095\n",
            "Epoch [4/4], Step [12/219], Loss: 0.3803\n",
            "Epoch [4/4], Step [14/219], Loss: 0.4387\n",
            "Epoch [4/4], Step [16/219], Loss: 0.4355\n",
            "Epoch [4/4], Step [18/219], Loss: 0.4337\n",
            "Epoch [4/4], Step [20/219], Loss: 0.4935\n",
            "Epoch [4/4], Step [22/219], Loss: 0.5330\n",
            "Epoch [4/4], Step [24/219], Loss: 0.4043\n",
            "Epoch [4/4], Step [26/219], Loss: 0.4410\n",
            "Epoch [4/4], Step [28/219], Loss: 0.5079\n",
            "Epoch [4/4], Step [30/219], Loss: 0.4598\n",
            "Epoch [4/4], Step [32/219], Loss: 0.4185\n",
            "Epoch [4/4], Step [34/219], Loss: 0.4047\n",
            "Epoch [4/4], Step [36/219], Loss: 0.4857\n",
            "Epoch [4/4], Step [38/219], Loss: 0.5033\n",
            "Epoch [4/4], Step [40/219], Loss: 0.4312\n",
            "Epoch [4/4], Step [42/219], Loss: 0.5079\n",
            "Epoch [4/4], Step [44/219], Loss: 0.4476\n",
            "Epoch [4/4], Step [46/219], Loss: 0.5830\n",
            "Epoch [4/4], Step [48/219], Loss: 0.4498\n",
            "Epoch [4/4], Step [50/219], Loss: 0.4296\n",
            "Epoch [4/4], Step [52/219], Loss: 0.4986\n",
            "Epoch [4/4], Step [54/219], Loss: 0.4331\n",
            "Epoch [4/4], Step [56/219], Loss: 0.5076\n",
            "Epoch [4/4], Step [58/219], Loss: 0.4187\n",
            "Epoch [4/4], Step [60/219], Loss: 0.4026\n",
            "Epoch [4/4], Step [62/219], Loss: 0.4222\n",
            "Epoch [4/4], Step [64/219], Loss: 0.4180\n",
            "Epoch [4/4], Step [66/219], Loss: 0.5290\n",
            "Epoch [4/4], Step [68/219], Loss: 0.4301\n",
            "Epoch [4/4], Step [70/219], Loss: 0.3650\n",
            "Epoch [4/4], Step [72/219], Loss: 0.4370\n",
            "Epoch [4/4], Step [74/219], Loss: 0.3813\n",
            "Epoch [4/4], Step [76/219], Loss: 0.4207\n",
            "Epoch [4/4], Step [78/219], Loss: 0.3615\n",
            "Epoch [4/4], Step [80/219], Loss: 0.4472\n",
            "Epoch [4/4], Step [82/219], Loss: 0.4458\n",
            "Epoch [4/4], Step [84/219], Loss: 0.4253\n",
            "Epoch [4/4], Step [86/219], Loss: 0.4311\n",
            "Epoch [4/4], Step [88/219], Loss: 0.4115\n",
            "Epoch [4/4], Step [90/219], Loss: 0.4090\n",
            "Epoch [4/4], Step [92/219], Loss: 0.4623\n",
            "Epoch [4/4], Step [94/219], Loss: 0.3899\n",
            "Epoch [4/4], Step [96/219], Loss: 0.4427\n",
            "Epoch [4/4], Step [98/219], Loss: 0.5015\n",
            "Epoch [4/4], Step [100/219], Loss: 0.4078\n",
            "Epoch [4/4], Step [102/219], Loss: 0.3960\n",
            "Epoch [4/4], Step [104/219], Loss: 0.4327\n",
            "Epoch [4/4], Step [106/219], Loss: 0.3867\n",
            "Epoch [4/4], Step [108/219], Loss: 0.3783\n",
            "Epoch [4/4], Step [110/219], Loss: 0.3843\n",
            "Epoch [4/4], Step [112/219], Loss: 0.4107\n",
            "Epoch [4/4], Step [114/219], Loss: 0.3961\n",
            "Epoch [4/4], Step [116/219], Loss: 0.4352\n",
            "Epoch [4/4], Step [118/219], Loss: 0.4436\n",
            "Epoch [4/4], Step [120/219], Loss: 0.3660\n",
            "Epoch [4/4], Step [122/219], Loss: 0.4055\n",
            "Epoch [4/4], Step [124/219], Loss: 0.4214\n",
            "Epoch [4/4], Step [126/219], Loss: 0.4697\n",
            "Epoch [4/4], Step [128/219], Loss: 0.4648\n",
            "Epoch [4/4], Step [130/219], Loss: 0.4095\n",
            "Epoch [4/4], Step [132/219], Loss: 0.3943\n",
            "Epoch [4/4], Step [134/219], Loss: 0.4131\n",
            "Epoch [4/4], Step [136/219], Loss: 0.4001\n",
            "Epoch [4/4], Step [138/219], Loss: 0.4330\n",
            "Epoch [4/4], Step [140/219], Loss: 0.4722\n",
            "Epoch [4/4], Step [142/219], Loss: 0.4554\n",
            "Epoch [4/4], Step [144/219], Loss: 0.4021\n",
            "Epoch [4/4], Step [146/219], Loss: 0.4077\n",
            "Epoch [4/4], Step [148/219], Loss: 0.4756\n",
            "Epoch [4/4], Step [150/219], Loss: 0.4024\n",
            "Epoch [4/4], Step [152/219], Loss: 0.5617\n",
            "Epoch [4/4], Step [154/219], Loss: 0.4628\n",
            "Epoch [4/4], Step [156/219], Loss: 0.4499\n",
            "Epoch [4/4], Step [158/219], Loss: 0.4477\n",
            "Epoch [4/4], Step [160/219], Loss: 0.3757\n",
            "Epoch [4/4], Step [162/219], Loss: 0.4751\n",
            "Epoch [4/4], Step [164/219], Loss: 0.3953\n",
            "Epoch [4/4], Step [166/219], Loss: 0.4765\n",
            "Epoch [4/4], Step [168/219], Loss: 0.3812\n",
            "Epoch [4/4], Step [170/219], Loss: 0.4651\n",
            "Epoch [4/4], Step [172/219], Loss: 0.4287\n",
            "Epoch [4/4], Step [174/219], Loss: 0.4271\n",
            "Epoch [4/4], Step [176/219], Loss: 0.4250\n",
            "Epoch [4/4], Step [178/219], Loss: 0.4717\n",
            "Epoch [4/4], Step [180/219], Loss: 0.3970\n",
            "Epoch [4/4], Step [182/219], Loss: 0.4502\n",
            "Epoch [4/4], Step [184/219], Loss: 0.3710\n",
            "Epoch [4/4], Step [186/219], Loss: 0.4094\n",
            "Epoch [4/4], Step [188/219], Loss: 0.4588\n",
            "Epoch [4/4], Step [190/219], Loss: 0.3729\n",
            "Epoch [4/4], Step [192/219], Loss: 0.4366\n",
            "Epoch [4/4], Step [194/219], Loss: 0.4245\n",
            "Epoch [4/4], Step [196/219], Loss: 0.3579\n",
            "Epoch [4/4], Step [198/219], Loss: 0.4111\n",
            "Epoch [4/4], Step [200/219], Loss: 0.4592\n",
            "Epoch [4/4], Step [202/219], Loss: 0.4059\n",
            "Epoch [4/4], Step [204/219], Loss: 0.4741\n",
            "Epoch [4/4], Step [206/219], Loss: 0.4050\n",
            "Epoch [4/4], Step [208/219], Loss: 0.4492\n",
            "Epoch [4/4], Step [210/219], Loss: 0.5518\n",
            "Epoch [4/4], Step [212/219], Loss: 0.3794\n",
            "Epoch [4/4], Step [214/219], Loss: 0.4402\n",
            "Epoch [4/4], Step [216/219], Loss: 0.4851\n",
            "Epoch [4/4], Step [218/219], Loss: 0.4337\n",
            "Epoch [4/4], Average Loss: 0.4333\n",
            "Epoch [4/4], Validation MSE: 0.4096, MAE: 0.4570, Pearson: 0.9071\n",
            "Checkpoint saved at ./ckpts/checkpoint_best.pth!\n",
            "Training complete. Best model found at epoch 4 with Pearson: 0.9071\n"
          ]
        }
      ],
      "source": [
        "!python train.py --config \"./config.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Testing\n",
        "\n",
        "After Stage II training completes, we perform evaluation on the test split.\n",
        "\n",
        "1. Prepare `Downstream/test_config.yaml`\n",
        "\n",
        "Key parameters:\n",
        "\n",
        "- `diffusion_path`: Same EMA checkpoint used during training.\n",
        "- `noise_label`: Must match the training configuration.\n",
        "- `out_dim`, `fusion_method`; Must be identical to the training setting, ensuring architectural consistency.\n",
        "- `fusion_net_path`: Path to the best checkpoint saved during Stage II training.\n",
        "\n",
        "\n",
        "## ‚úÖ Copy the following YAML to `Downstream/test_config.yaml`:\n",
        "\n",
        "```yaml\n",
        "# data\n",
        "test_patch_file: \"/content/data/HEST_bowel/test_patch.h5\"\n",
        "test_gene_file: \"/content/data/HEST_bowel/test_gene.h5\"\n",
        "\n",
        "# pretrained model\n",
        "diffusion_path: \"/content/DiffBulk/Pretrain/ema/phema-0000032-0.100.pkl\"\n",
        "\n",
        "# ckpt\n",
        "fusion_net_path: \"/content/DiffBulk/Downstream/ckpts/checkpoint_best.pth\"\n",
        "\n",
        "device: cuda\n",
        "batch_size: 32\n",
        "\n",
        "# fusion net architecture\n",
        "noise_label: 0.01\n",
        "out_dim: 541\n",
        "fusion_method: \"gated_residual\"\n",
        "c: 1.0\n",
        "c_learnable: True\n",
        "```"
      ],
      "metadata": {
        "id": "s3J2GBaXThr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Run Test Script**"
      ],
      "metadata": {
        "id": "7EM9LocTVJVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a_q06xzYk-0",
        "outputId": "222c0225-e669-4640-fb71-0cf0d3b97381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing model with config: ./test_config.yaml\n",
            "Testing using configuration: ./test_config.yaml\n",
            "2025-11-24 16:01:33.608469: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-24 16:01:33.626233: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764000093.647321   67227 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764000093.653783   67227 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764000093.670044   67227 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764000093.670074   67227 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764000093.670077   67227 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764000093.670079   67227 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-24 16:01:33.674951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "FusionNet Checkpoint loaded from /content/DiffBulk/Downstream/ckpts/checkpoint_best.pth!\n",
            "Restored c value: 1.0\n",
            "Pearson Correlation Coefficient: 0.4111\n",
            "P-value: 0.0000e+00\n",
            "MAE: 1.1308\n",
            "MSE: 2.4546\n",
            "Results appended to ./results.csv\n",
            "All tests completed. Results saved to ./results.csv\n"
          ]
        }
      ],
      "source": [
        "! bash test.sh"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}